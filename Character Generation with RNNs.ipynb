{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c27a06-3a7d-4b09-9126-082f0fb74183",
   "metadata": {},
   "source": [
    "# GOAL: Text Generation using RNNs.\n",
    "Given a sequence of characters, the task is to train a model which can predict the next character that occurs.\n",
    "\n",
    "**Note:** This notebook is almost a direct replica of Tensorflow tutorial. Intended for practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf845af8-9d4a-466f-997f-534dbae81250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing essential libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2be86003-02a4-45ba-b105-85aeba2ec456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e1efe-4cd7-4a38-bb62-dc1bafcb41dd",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18aff0cc-9811-4255-bd0c-c3661c31e027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ichbi\\.keras\\datasets\\shakespeare.txt\n"
     ]
    }
   ],
   "source": [
    "# Download the shakespere's text file\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba873efd-c0d7-46ee-ab8b-c4d01f7a09be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.BufferedReader'>\n",
      "<class 'str'>\n",
      "Length of text: 1115394 characters.\n"
     ]
    }
   ],
   "source": [
    "# Read the file as a buffered reader\n",
    "text = open(path_to_file, \"rb\"); print(type(text))\n",
    "# Read the text file through the buffered reader and decode into UTF-8 format for python to read it in a formatted manner.\n",
    "text = text.read().decode(encoding=\"utf-8\"); print(type(text));\n",
    "# Display the length of characters\n",
    "print(f\"Length of text: {len(text)} characters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "726b15c7-5c01-4f40-94ba-a4fbb7929c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first few lines of the shakespere's play\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4648aa07-2a86-4386-97dd-a86e60a47c30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65 unique characters.\n"
     ]
    }
   ],
   "source": [
    "# Break all the words and store the unique characters.\n",
    "## RESULT: VOCABULARY OF CHARACTERS\n",
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "print(f\"{len(vocab)} unique characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1ae68-38fa-4dfb-b839-512200b2fa54",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcad92e9-23be-4fbb-b4c1-eb2ba4223722",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef1152d-c52d-4140-8f82-51723f244a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample examples to test out if the tokeization is successful\n",
    "example_texts = [\"today I will be learning natural language processing\", \"okay thats it\"]\n",
    "\n",
    "# Divde the sentence into characters for tokenization\n",
    "characters = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cf2f3fa-7a10-4ba2-8452-5814afcc71b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[59, 54, 43, 40, 64, 2, 22, 2, 62, 48, 51, 51, 2, 41, 44, 2, 51, 44, 40,\n",
      "  57, 53, 48, 53, 46, 2, 53, 40, 59, 60, 57, 40, 51, 2, 51, 40, 53, 46, 60,\n",
      "  40, 46, 44, 2, 55, 57, 54, 42, 44, 58, 58, 48, 53, 46]                   ,\n",
      " [54, 50, 40, 64, 2, 59, 47, 40, 59, 58, 2, 48, 59]]>\n"
     ]
    }
   ],
   "source": [
    "# Lookup table to convert the characters to its corresponding numerical format\n",
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n",
    "\n",
    "# Finally, generate integer mapping for the characters from the examples to get a tokenized output\n",
    "## RESULT: TOKENIZED RAGGED TENSOR, with each row containing integer mappings for corresponding characters in the sentences. \n",
    "ids = ids_from_chars(characters)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5019ac96-9710-41f4-8fcb-50957f1327af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'today I will be learning natural language processing' b'okay thats it']\n"
     ]
    }
   ],
   "source": [
    "# To obtain a mapping from the encoded integer mapping back to string, create an invertible lookup table\n",
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary = ids_from_chars.get_vocabulary(), mask_token = None, invert = True)\n",
    "\n",
    "# Testing the inverse from vector to string and converting to words\n",
    "## RESULTS: CHARACTERS converted from integers. HOPEFULLY ORIGINAL SENTENCES if inverse mapping worked\n",
    "chars = chars_from_ids(ids)\n",
    "print(tf.strings.reduce_join(chars, axis = -1).numpy())\n",
    "\n",
    "# Write a function to later use it for joining the characters and reading the sentence\n",
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis = -1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70bddd8a-d318-4582-a0ca-5296c72f6853",
   "metadata": {},
   "source": [
    "# Preparing the data to create training sequences for the model\n",
    "\n",
    "We now divide the text into example sequences. Each input sequence should contain **sequence_length** number of characters. \n",
    "\n",
    "The corresponding target contains the same length of text, except shifted one character to the right.\n",
    "\n",
    "Now, we break the chunk of text into seq_length+1 size.\n",
    "    \n",
    "    e.g., if **seq_length** = 4\n",
    "          input_text = \"hello\"\n",
    "          Then, sequence would be: \"hell\" and target sequence will be: \"ello\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4962005-170b-42fd-ae72-b8b724ad70d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "z\n",
      "e\n",
      "n\n",
      ":\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# So we essentially used the vocabulary we created using the shakespere's play by tokenizing them.\n",
    "## and we later used the encoding (i.e., StringLookup) object to encode the orignal play's characters.\n",
    "## OUTPUT: TOKENIZED SHAKESPEARES PLAY TEXT\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, input_encoding=\"UTF-8\"))\n",
    "all_ids\n",
    "\n",
    "# Creates a Tensorflow's dataset object through all the tensor slices created. (i.e., no of lists containing sentences)\n",
    "## i.e., if two sentences were provided for encoding, two different lists are created. Hence, two internal \n",
    "## sets are created in dataset object, preserving the data structure. \n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
    "\n",
    "# Displaying the first 15 characters from the created TF dataset object\n",
    "for ids in ids_dataset.take(15):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"UTF-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffa54a8d-3b0e-4d60-adc3-b1bea3713868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "# Sequence length for providing context to the model for learning the probabilities of next characters.\n",
    "seq_length = 100\n",
    "# add it as a batch size to output seq_len number of characters at once.\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# Test the changes\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))\n",
    "\n",
    "# Merge the characters to form sentences\n",
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e891be-5117-4c5b-807b-de0f716f7538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eec8096-3668-47a3-839f-593e1af88270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# Function to split the sequences into its corresponding input and target sequences.\n",
    "## OUTPUT: INPUT and TARGET both of length seq_len to be later fed into the model\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:seq_length+1]\n",
    "    return input_text, target_text\n",
    "\n",
    "# Testing\n",
    "split_input_target(list(\"Tensorflow\"))\n",
    "\n",
    "# Mapping this function to the sequence generator to be used in TensorFlow RNN model\n",
    "sequence_dataset = sequences.map(split_input_target)\n",
    "\n",
    "for input_example, target_example in sequence_dataset.take(1):\n",
    "    print(\"input\", text_from_ids(input_example))\n",
    "    print(\"Target\", text_from_ids(target_example))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d52947-a8e3-4d2d-8b04-b3c6d2cf68f7",
   "metadata": {},
   "source": [
    "# Actual Training Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ecce2fb-8594-43ed-ae8e-4bceab79a699",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset element_spec=(TensorSpec(shape=(16, 100), dtype=tf.int64, name=None), TensorSpec(shape=(16, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch Size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "model_dataset = (\n",
    "    sequence_dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "model_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdb30e61-f240-441f-bb40-13f1a88a7312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size  = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "403956ee-9758-4c67-8387-861749e4b945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embeddings = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state = True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states = None, return_state=False, training = False):\n",
    "        x = inputs\n",
    "        x = self.embeddings(x, training = training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training = training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "765d4257-6310-4c8c-a82c-2b768dcb430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(\n",
    "    vocab_size = vocab_size,\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41470aa-bd55-4bb2-bd54-6cb277a5056f",
   "metadata": {},
   "source": [
    "# Try out the model without training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b58c001-5f7a-497e-91e5-5442cc24ddf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in model_dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e31dd31-bcd5-4798-b91d-2d2dfb1c9a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d45692ae-4891-40f3-95dd-0641326b41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3572f9cb-3f39-4975-984c-ee6bd467bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([43, 16, 15, 39, 41, 14, 21, 33, 15, 53, 12, 63, 40,  3, 64, 11, 42,\n",
       "       10, 32,  7, 44, 15, 11,  8, 53, 41, 30, 62, 39,  2, 29, 50,  1, 39,\n",
       "       35, 55, 14, 58, 47, 54, 39, 20, 37, 54,  2, 32, 34,  6,  4,  7, 65,\n",
       "       35, 17, 42, 12, 17, 34, 40, 29, 13, 59, 59, 32, 39, 56, 42, 22,  1,\n",
       "       13, 65, 12, 24,  4, 32, 18, 60, 35, 24, 40, 60, 47, 17, 57, 37, 34,\n",
       "       32, 41,  8, 60, 50, 21, 41, 53, 60, 26, 63, 64,  0, 28, 48],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66435bb7-d44b-487d-8377-4dabeac85132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"uch a name,\\nWhose repetition will be dogg'd with curses;\\nWhose chronicle thus writ: 'The man was nob\"\n",
      "b\"dCBZbAHTBn;xa!y:c3S,eB:-nbQwZ Pk\\nZVpAshoZGXo SU'$,zVDc;DUaP?ttSZqcI\\n?z;K$SEuVKauhDrXUSb-ukHbnuMxy[UNK]Oi\"\n"
     ]
    }
   ],
   "source": [
    "print(text_from_ids(input_example_batch[0]))\n",
    "print(text_from_ids(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b527d4e3-0dff-45a3-935f-948d19aaacf7",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0c0aae2-793c-4e71-af89-dc3b6e2db542",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65b2dca2-dacc-4d32-8349-659ba767a13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (16, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1889067, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "760ad5a7-70a1-4b33-9433-3e2d5c1dc947",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss = loss)\n",
    "\n",
    "# Directory where the checkpoints are saved\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "# Name of checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_prefix,\n",
    "    save_weights_only = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ec57a69-6163-4c49-9a91-4aa9fcb7e1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.3332\n",
      "Epoch 2/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.2648\n",
      "Epoch 3/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.2030\n",
      "Epoch 4/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.1435\n",
      "Epoch 5/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.0830\n",
      "Epoch 6/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 1.0226\n",
      "Epoch 7/20\n",
      "690/690 [==============================] - 12s 17ms/step - loss: 0.9669\n",
      "Epoch 8/20\n",
      "690/690 [==============================] - 13s 17ms/step - loss: 0.9167\n",
      "Epoch 9/20\n",
      "690/690 [==============================] - 13s 17ms/step - loss: 0.8756\n",
      "Epoch 10/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.8438\n",
      "Epoch 11/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.8202\n",
      "Epoch 12/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.8051\n",
      "Epoch 13/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.7921\n",
      "Epoch 14/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.7882\n",
      "Epoch 15/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.7909\n",
      "Epoch 16/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.7940\n",
      "Epoch 17/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.7990\n",
      "Epoch 18/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.8037\n",
      "Epoch 19/20\n",
      "690/690 [==============================] - 13s 18ms/step - loss: 0.8131\n",
      "Epoch 20/20\n",
      "690/690 [==============================] - 13s 19ms/step - loss: 0.8293\n"
     ]
    }
   ],
   "source": [
    "##### EPOCHS = 20\n",
    "\n",
    "history = model.fit(model_dataset, epochs = EPOCHS, callbacks = [checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4b213724-18bc-42da-9a2d-2b61cfa43e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to run the model on \n",
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.5):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    #predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "080bdb24-1102-47b7-a737-8b55a2059913",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "30853e86-1907-4799-9c5e-6822c5df79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "A fellow  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.12838125228881836\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Siv:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(10):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad040e-f8cc-4729-87df-9e3fae143d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
